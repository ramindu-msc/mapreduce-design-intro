{"paragraphs":[{"text":"%md\n# Creating DataFrames","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Creating DataFrames</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713922_-1242920865","id":"20161121-211804_261745742","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:238"},{"text":"%pyspark\n\n# spark is an existing SparkSession\ndf = spark.read.json(\"/data/people.json\")\n# Displays the content of the DataFrame to stdout\ndf.show()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+","user":"anonymous","dateUpdated":"2019-11-25T15:10:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{"name":"","spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1574691713935_-1140091233","id":"20161121-211505_204906499","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:239","dateFinished":"2019-11-25T15:10:10+0000","dateStarted":"2019-11-25T15:10:10+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=34","http://172.17.0.2:4040/jobs/job?id=35"],"interpreterSettingId":"spark"}}},{"text":"%md\n# Untyped Dataset Operations (aka DataFrame Operations)","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Untyped Dataset Operations (aka DataFrame Operations)</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713936_-493426139","id":"20161121-212243_1042559294","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:240"},{"text":"%pyspark\n\n# spark, df are from the previous example\n# Print the schema in a tree format\ndf.printSchema()\n# root\n# |-- age: long (nullable = true)\n# |-- name: string (nullable = true)\n\n# Select only the \"name\" column\ndf.select(\"name\").show()\n# +-------+\n# |   name|\n# +-------+\n# |Michael|\n# |   Andy|\n# | Justin|\n# +-------+\n\n# Select everybody, but increment the age by 1\ndf.select(df['name'], df['age'] + 1).show()\n# +-------+---------+\n# |   name|(age + 1)|\n# +-------+---------+\n# |Michael|     null|\n# |   Andy|       31|\n# | Justin|       20|\n# +-------+---------+\n\n# Select people older than 21\ndf.filter(df['age'] > 21).show()\n# +---+----+\n# |age|name|\n# +---+----+\n# | 30|Andy|\n# +---+----+\n\n# Count people by age\ndf.groupBy(\"age\").count().show()\n# +----+-----+\n# | age|count|\n# +----+-----+\n# |  19|    1|\n# |null|    1|\n# |  30|    1|\n# +----+-----+","user":"anonymous","dateUpdated":"2019-11-25T14:46:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n+-------+\n|   name|\n+-------+\n|Michael|\n|   Andy|\n| Justin|\n+-------+\n\n+-------+---------+\n|   name|(age + 1)|\n+-------+---------+\n|Michael|     null|\n|   Andy|       31|\n| Justin|       20|\n+-------+---------+\n\n+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n\n+----+-----+\n| age|count|\n+----+-----+\n|  19|    1|\n|null|    1|\n|  30|    1|\n+----+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1574691713936_-179665506","id":"20161121-212329_269221065","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:241","dateFinished":"2019-11-25T14:46:18+0000","dateStarted":"2019-11-25T14:46:13+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=7","http://172.17.0.2:4040/jobs/job?id=8","http://172.17.0.2:4040/jobs/job?id=9","http://172.17.0.2:4040/jobs/job?id=10","http://172.17.0.2:4040/jobs/job?id=11","http://172.17.0.2:4040/jobs/job?id=12","http://172.17.0.2:4040/jobs/job?id=13","http://172.17.0.2:4040/jobs/job?id=14"],"interpreterSettingId":"spark"}}},{"text":"%md\n# Running SQL Queries Programmatically","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Running SQL Queries Programmatically</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713937_538596816","id":"20161121-212510_1424611231","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:242"},{"text":"%pyspark\n\n# Register the DataFrame as a SQL temporary view\ndf.createOrReplaceTempView(\"people\")\n\nsqlDF = spark.sql(\"SELECT * FROM people\")\nsqlDF.show()\n# +----+-------+\n# | age|   name|\n# +----+-------+\n# |null|Michael|\n# |  30|   Andy|\n# |  19| Justin|\n# +----+-------+","user":"anonymous","dateUpdated":"2019-11-25T15:09:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1574691713938_1705893621","id":"20161121-212653_1761768042","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:243","dateFinished":"2019-11-25T15:09:52+0000","dateStarted":"2019-11-25T15:09:52+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=33"],"interpreterSettingId":"spark"}}},{"text":"%sql\nSELECT * FROM people","user":"anonymous","dateUpdated":"2019-11-25T14:48:31+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"age":"string","name":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"age","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"name","index":1,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574693258498_-736822991","id":"20191125-144738_1887567369","dateCreated":"2019-11-25T14:47:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3685","dateFinished":"2019-11-25T14:47:56+0000","dateStarted":"2019-11-25T14:47:56+0000","results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\tname\nnull\tMichael\n30\tAndy\n19\tJustin\n"},{"type":"TEXT","data":""}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=16"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n# Creating Datasets","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Creating Datasets</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713939_-1255359810","id":"20161121-212717_908245571","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:244"},{"text":"%spark\n\nimport spark.implicits._\n// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,\n// you can use custom classes that implement the Product interface\ncase class Person(name: String, age: Long)\n\n// Encoders are created for case classes\nval caseClassDS = Seq(Person(\"Andy\", 32), Person(\"Andy1\", 33)).toDS()\ncaseClassDS.show()\n// +----+---+\n// |name|age|\n// +----+---+\n// |Andy| 32|\n// +----+---+\n\n// Encoders for most common types are automatically provided by importing spark.implicits._\nval primitiveDS = Seq(1, 2, 3).toDS()\nprimitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)\n\n// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\nval path =\"/data/people.json\"\nval peopleDS = spark.read.json(path).as[Person]\npeopleDS.show()\n// +----+-------+\n// | age|   name|\n// +----+-------+\n// |null|Michael|\n// |  30|   Andy|\n// |  19| Justin|\n// +----+-------+","user":"anonymous","dateUpdated":"2019-11-25T14:56:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+---+\n| name|age|\n+-----+---+\n| Andy| 32|\n|Andy1| 33|\n+-----+---+\n\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\nimport spark.implicits._\ndefined class Person\ncaseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]\nprimitiveDS: org.apache.spark.sql.Dataset[Int] = [value: int]\npath: String = /data/people.json\npeopleDS: org.apache.spark.sql.Dataset[Person] = [age: bigint, name: string]\n"}]},"apps":[],"jobName":"paragraph_1574691713939_1167884901","id":"20161121-213642_898338092","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:245","dateFinished":"2019-11-25T14:56:51+0000","dateStarted":"2019-11-25T14:56:49+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=20","http://172.17.0.2:4040/jobs/job?id=21","http://172.17.0.2:4040/jobs/job?id=22"],"interpreterSettingId":"spark"}}},{"text":"%md \n# Inferring the Schema Using Reflection","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Inferring the Schema Using Reflection</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713940_-20551191","id":"20161121-213745_455199652","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:246"},{"text":"%pyspark\n\nfrom pyspark.sql import *\n\n# Load a text file and convert each line to a Row.\nlines = sc.textFile(\"/data/people.txt\")\nprint(lines.collect())\n\nparts = lines.map(lambda l: l.split(\",\"))\npeople = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n\n\n# Infer the schema, and register the DataFrame as a table.\nschemaPeople = spark.createDataFrame(people)\nschemaPeople.createOrReplaceTempView(\"people\")\n\n# SQL can be run over DataFrames that have been registered as a table.\nteenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n\n# The results of SQL queries are Dataframe objects.\n# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\nteenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\nfor name in teenNames:\n    print(name)\n# Name: Justin","user":"anonymous","dateUpdated":"2019-11-25T15:23:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[u'Michael, 29', u'Andy, 30', u'Justin, 19']\nName: Justin\n"}]},"apps":[],"jobName":"paragraph_1574691713940_924651071","id":"20161121-214219_563939179","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:247","dateFinished":"2019-11-25T15:23:41+0000","dateStarted":"2019-11-25T15:23:41+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=63","http://172.17.0.2:4040/jobs/job?id=64","http://172.17.0.2:4040/jobs/job?id=65"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n# Programmatically Specifying the Schema","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Programmatically Specifying the Schema</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713941_-684862921","id":"20161121-214254_1160849755","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:248"},{"text":"%pyspark\n\n# Import data types\nfrom pyspark.sql.types import *\n\n\n# Load a text file and convert each line to a Row.\nlines = sc.textFile(\"/data/people.txt\")\nparts = lines.map(lambda l: l.split(\",\"))\n# Each line is converted to a tuple.\npeople = parts.map(lambda p: (p[0], p[1].strip()))\n\n# The schema is encoded in a string.\nschemaString = \"name age\"\n\nfields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\nschema = StructType(fields)\n\n# Apply the schema to the RDD.\nschemaPeople = spark.createDataFrame(people, schema)\n\n# Creates a temporary view using the DataFrame\nschemaPeople.createOrReplaceTempView(\"people\")\n\n# SQL can be run over DataFrames that have been registered as a table.\nresults = spark.sql(\"SELECT name FROM people\")\n\nresults.show()\n# +-------+\n# |   name|\n# +-------+\n# |Michael|\n# |   Andy|\n# | Justin|\n# +-------+","user":"anonymous","dateUpdated":"2019-11-25T15:14:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+\n|   name|\n+-------+\n|Michael|\n|   Andy|\n| Justin|\n+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1574691713942_-150298722","id":"20161121-215434_1308115320","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:249","dateFinished":"2019-11-25T15:14:56+0000","dateStarted":"2019-11-25T15:14:56+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=41"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n# Data Sources","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Data Sources</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713942_1962635640","id":"20161121-215556_1513433460","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:250"},{"text":"%md \n# Generic Load/Save Functions","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Generic Load/Save Functions</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713943_1096812239","id":"20161121-221300_1709976506","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:251"},{"text":"%pyspark\n\nimport shutil\nshutil.rmtree('namesAndFavColors.parquet', ignore_errors=True)\n\ndf = spark.read.load(\"/data/users.parquet\")\ndf.show()\ndf.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")","user":"anonymous","dateUpdated":"2019-11-25T15:17:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n"}]},"apps":[],"jobName":"paragraph_1574691713943_665084564","id":"20161121-221324_1792501703","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:252","dateFinished":"2019-11-25T15:17:04+0000","dateStarted":"2019-11-25T15:17:03+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=42","http://172.17.0.2:4040/jobs/job?id=43","http://172.17.0.2:4040/jobs/job?id=44"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\ndf = spark.read.load(\"namesAndFavColors.parquet\")\ndf.show()","user":"anonymous","dateUpdated":"2019-11-25T15:17:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------------+\n|  name|favorite_color|\n+------+--------------+\n|Alyssa|          null|\n|   Ben|           red|\n+------+--------------+\n\n"}]},"apps":[],"jobName":"paragraph_1574691713943_1703551406","id":"20161121-221352_253824984","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:253","dateFinished":"2019-11-25T15:17:49+0000","dateStarted":"2019-11-25T15:17:48+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=45","http://172.17.0.2:4040/jobs/job?id=46"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\nimport shutil\nshutil.rmtree('namesAndAges.parquet', ignore_errors=True)\n\n\ndf = spark.read.load(\"/data/people.json\", format=\"json\")\ndf.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")\n\ndf = spark.read.load(\"namesAndAges.parquet\")\ndf.show()","user":"anonymous","dateUpdated":"2019-11-25T15:19:07+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+----+\n|   name| age|\n+-------+----+\n|Michael|null|\n|   Andy|  30|\n| Justin|  19|\n+-------+----+\n\n"}]},"apps":[],"jobName":"paragraph_1574691713944_-419111609","id":"20161121-221523_1944730815","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:254","dateFinished":"2019-11-25T15:19:08+0000","dateStarted":"2019-11-25T15:19:07+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=47","http://172.17.0.2:4040/jobs/job?id=48","http://172.17.0.2:4040/jobs/job?id=49","http://172.17.0.2:4040/jobs/job?id=50"],"interpreterSettingId":"spark"}}},{"text":"%pyspark \n\ndf = spark.sql(\"SELECT * FROM parquet.`namesAndAges.parquet`\")\ndf.show()","user":"anonymous","dateUpdated":"2019-11-25T15:20:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+----+\n|   name| age|\n+-------+----+\n|Michael|null|\n|   Andy|  30|\n| Justin|  19|\n+-------+----+\n\n"}]},"apps":[],"jobName":"paragraph_1574691713945_704823683","id":"20161121-221645_1477557536","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:255","dateFinished":"2019-11-25T15:20:01+0000","dateStarted":"2019-11-25T15:20:00+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=53","http://172.17.0.2:4040/jobs/job?id=54"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n# Loading Data Programmatically","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Loading Data Programmatically</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713945_-1319048671","id":"20161121-222620_202866957","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:256"},{"text":"%pyspark\n\nimport shutil\nshutil.rmtree(\"people.parquet\", ignore_errors=True)\n\npeopleDF = spark.read.json(\"/data/people.json\")\n\n# DataFrames can be saved as Parquet files, maintaining the schema information.\npeopleDF.write.parquet(\"people.parquet\")\n\n# Read in the Parquet file created above.\n# Parquet files are self-describing so the schema is preserved.\n# The result of loading a parquet file is also a DataFrame.\nparquetFile = spark.read.parquet(\"people.parquet\")\n\n# Parquet files can also be used to create a temporary view and then used in SQL statements.\nparquetFile.createOrReplaceTempView(\"parquetFile\")\nteenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\nteenagers.show()\n# +------+\n# |  name|\n# +------+\n# |Justin|\n# +------+","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{"spark_home":"/Users/suho/Documents/msc/iit/big-data-programming/sessions/week9/lab/spark-2.0.1-bin-hadoop2.7"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+\n|  name|\n+------+\n|Justin|\n+------+\n\n"}]},"apps":[],"jobName":"paragraph_1574691713945_312524583","id":"20161121-223150_132443134","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:257"},{"text":"%md\n\n# Schema Merging","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Schema Merging</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713946_-1030776866","id":"20161121-223224_183585777","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:258"},{"text":"%pyspark\n\nimport shutil\nshutil.rmtree('data', ignore_errors=True)\n\n# from pyspark.sql import Row\n\n# spark is from the previous example.\n# Create a simple DataFrame, stored into a partition directory\nsc = spark.sparkContext\n\nsquaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\n                                  .map(lambda i: Row(single=i, double=i ** 2)))\nsquaresDF.write.parquet(\"data/test_table/key=1\")\n\n# Create another DataFrame in a new partition directory,\n# adding a new column and dropping an existing column\ncubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n                                .map(lambda i: Row(single=i, triple=i ** 3)))\ncubesDF.write.parquet(\"data/test_table/key=2\")\n\n# Read the partitioned table\nmergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\nmergedDF.printSchema()\n\n# The final schema consists of all 3 columns in the Parquet files together\n# with the partitioning column appeared in the partition directory paths.\n# root\n#  |-- double: long (nullable = true)\n#  |-- single: long (nullable = true)\n#  |-- triple: long (nullable = true)\n#  |-- key: integer (nullable = true)","user":"anonymous","dateUpdated":"2019-11-25T15:23:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- double: long (nullable = true)\n |-- single: long (nullable = true)\n |-- triple: long (nullable = true)\n |-- key: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1574691713946_-1380772816","id":"20161121-223620_204262642","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:259","dateFinished":"2019-11-25T15:23:05+0000","dateStarted":"2019-11-25T15:23:04+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=55","http://172.17.0.2:4040/jobs/job?id=56","http://172.17.0.2:4040/jobs/job?id=57","http://172.17.0.2:4040/jobs/job?id=58","http://172.17.0.2:4040/jobs/job?id=59"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\nmergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\nmergedDF.show()","user":"anonymous","dateUpdated":"2019-11-25T15:24:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+------+------+---+\n|double|single|triple|key|\n+------+------+------+---+\n|  null|     6|   216|  2|\n|  null|     7|   343|  2|\n|  null|     8|   512|  2|\n|  null|     9|   729|  2|\n|  null|    10|  1000|  2|\n|     1|     1|  null|  1|\n|     4|     2|  null|  1|\n|     9|     3|  null|  1|\n|    16|     4|  null|  1|\n|    25|     5|  null|  1|\n+------+------+------+---+\n\n"}]},"apps":[],"jobName":"paragraph_1574691713947_2000550624","id":"20161121-223720_479563238","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:260","dateFinished":"2019-11-25T15:24:19+0000","dateStarted":"2019-11-25T15:24:19+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=66","http://172.17.0.2:4040/jobs/job?id=67"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n# Spark SQL","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Spark SQL</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1574691713947_-1992175385","id":"20161121-224455_578990309","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:261"},{"text":"%sql\nshow tables","user":"anonymous","dateUpdated":"2019-11-25T15:25:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{},"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"database":"string","tableName":"string","isTemporary":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}],"enabled":true,"editorSetting":{"language":"sql"},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"database\ttableName\tisTemporary\n\tpeople\ttrue\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1574691713947_-231346446","id":"20161121-224034_728861532","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:262","dateFinished":"2019-11-25T15:25:00+0000","dateStarted":"2019-11-25T15:25:00+0000"},{"text":"%sql \n\nselect name, age from people where age > 0","user":"anonymous","dateUpdated":"2019-11-25T15:25:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"age","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"age","index":1,"aggr":"sum"}},"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"name":"string","age":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}},"helium":{}}],"enabled":true,"editorSetting":{"language":"sql"},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\nMichael\t29\nAndy\t30\nJustin\t19\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1574691713948_1092806078","id":"20161121-224655_1255434563","dateCreated":"2019-11-25T14:21:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:263","dateFinished":"2019-11-25T15:25:16+0000","dateStarted":"2019-11-25T15:25:16+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=68"],"interpreterSettingId":"spark"}}},{"text":"","user":"anonymous","dateUpdated":"2019-11-25T14:21:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574691713948_1819662249","id":"20161121-224712_2007083219","dateCreated":"2019-11-25T14:21:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:264"}],"name":"Spark Lab Session","id":"2EST5J56C","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}